{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "DATA_FILE = 'E:/Academics/data/function_assignment/function_assignment.csv'\n",
    "df = pd.read_csv(DATA_FILE,encoding='latin-1')\n",
    "train, test = train_test_split(df, train_size = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = train.function\n",
    "texts = train.job_title\n",
    "tags2 = test.function\n",
    "texts2 = test.job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_max = 1000\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags)\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_texts = tok.texts_to_matrix(texts,mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(mat_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9130,) (9130, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(tags.shape,mat_texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  1]\n",
      "(9130, 100)\n"
     ]
    }
   ],
   "source": [
    "max_len = 100\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "print(cnn_texts_seq[0])\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n",
    "print(cnn_texts_mat[0])\n",
    "print(cnn_texts_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_max = 1000\n",
    "le2 = LabelEncoder()\n",
    "tags2 = le.fit_transform(tags2)\n",
    "tok2 = Tokenizer(num_words=num_max)\n",
    "tok2.fit_on_texts(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_texts2 = tok.texts_to_matrix(texts2,mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_texts_seq2 = tok.texts_to_sequences(texts2)\n",
    "cnn_texts_mat2 = sequence.pad_sequences(cnn_texts_seq2,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9130, 326)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "lab_train = to_categorical(tags)\n",
    "lab_test = to_categorical(tags2)\n",
    "\n",
    "lab_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Academics\\Anaconda\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7304 samples, validate on 1826 samples\n",
      "Epoch 1/10\n",
      "7304/7304 [==============================] - 20s 3ms/step - loss: 4.6094 - acc: 0.1083 - val_loss: 3.9895 - val_acc: 0.1292\n",
      "Epoch 2/10\n",
      "7304/7304 [==============================] - 18s 2ms/step - loss: 3.8989 - acc: 0.1104 - val_loss: 3.9379 - val_acc: 0.1292\n",
      "Epoch 3/10\n",
      "7304/7304 [==============================] - 19s 3ms/step - loss: 3.7917 - acc: 0.1006 - val_loss: 3.8030 - val_acc: 0.0827\n",
      "Epoch 4/10\n",
      "7304/7304 [==============================] - 18s 2ms/step - loss: 3.6428 - acc: 0.1008 - val_loss: 3.7398 - val_acc: 0.0876\n",
      "Epoch 5/10\n",
      "7304/7304 [==============================] - 18s 2ms/step - loss: 3.5106 - acc: 0.1015 - val_loss: 3.6067 - val_acc: 0.1391\n",
      "Epoch 6/10\n",
      "7304/7304 [==============================] - 19s 3ms/step - loss: 3.3993 - acc: 0.1183 - val_loss: 3.5125 - val_acc: 0.1298\n",
      "Epoch 7/10\n",
      "7304/7304 [==============================] - 18s 2ms/step - loss: 3.2251 - acc: 0.1777 - val_loss: 3.2827 - val_acc: 0.2448\n",
      "Epoch 8/10\n",
      "7304/7304 [==============================] - 18s 2ms/step - loss: 3.0243 - acc: 0.2601 - val_loss: 3.1477 - val_acc: 0.2990\n",
      "Epoch 9/10\n",
      "7304/7304 [==============================] - 18s 2ms/step - loss: 2.8361 - acc: 0.3131 - val_loss: 3.0186 - val_acc: 0.3308\n",
      "Epoch 10/10\n",
      "7304/7304 [==============================] - 18s 2ms/step - loss: 2.6887 - acc: 0.3413 - val_loss: 2.8935 - val_acc: 0.3516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d502211dd8>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "def cnn_model(): \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(256,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(326))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    rms = optimizers.RMSprop(lr=0.001, rho=0.9, decay=1e-2)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "m = cnn_model()\n",
    "m.fit(cnn_texts_mat,lab_train, batch_size = 200, nb_epoch = 10, validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
